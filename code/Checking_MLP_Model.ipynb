{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Name\n",
    "MultI-Task  Ordinal  Regression  (MITOR)  \n",
    "Spatial  In-complete Multi-task Deep leArning (SIMDA)  \n",
    "Pending Task: Deciding the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Incomplete_label_multi_task\\lib\\site-packages\\scipy\\io\\matlab\\mio.py:226: MatReadWarning: Duplicate variable name \"None\" in stream - replacing previous with new\n",
      "Consider mio5.varmats_from_mat to split file into single variable files\n",
      "  matfile_dict = MR.get_variables(variable_names)\n"
     ]
    }
   ],
   "source": [
    "# Reading Data\n",
    "import scipy.io\n",
    "import utils\n",
    "import configure as config\n",
    "# data processing\n",
    "config.write_params()\n",
    "data = scipy.io.loadmat(config.data_location) # data format: dict\n",
    "[X_train, Y_train, X_test, Y_test, adj_data] = utils.data_formulate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# MLP Model\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2=0):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim_1 = hidden_dim_1\n",
    "        self.fc1 = torch.nn.Linear(self.input_dim, self.hidden_dim_1)\n",
    "        self.fc1.weight.data.copy_(torch.eye(self.hidden_dim_1)) # weight initialization by identity matrix\n",
    "        self.fc1.bias.data.fill_(0) # weight initialization by zero\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        if hidden_dim_2!=0:\n",
    "            self.hidden_dim_2 = hidden_dim_2\n",
    "            self.fc2 = torch.nn.Linear(self.hidden_dim_1, self.hidden_dim_2)\n",
    "        else:self.hidden_dim_2 = None\n",
    "    def forward(self, x):\n",
    "        if self.hidden_dim_2 != None:\n",
    "            if config.activation_function == 'relu':\n",
    "                X_hidden = F.relu(self.fc2(self.fc1(x)))\n",
    "            elif config.activation_function == 'selu':\n",
    "                X_hidden = F.relu(self.fc2(self.fc1(x)))\n",
    "        else:\n",
    "            if config.activation_function == 'relu':\n",
    "                # X_hidden = self.dropout(F.selu(self.fc1(x))) # relu\n",
    "                X_hidden = torch.relu(self.fc1(x))  # relu\n",
    "                # X_hidden = self.fc1(x)\n",
    "            elif config.activation_function == 'selu':\n",
    "                X_hidden = torch.selu(self.fc1(x))  # relu\n",
    "\n",
    "        return X_hidden\n",
    "mlp_model = MLP(config.input_dim, config.hidden_dim_1, config.hidden_dim_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight\n",
      "fc1.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in mlp_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "#         print(name, param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.mlp_optimizer=='sgd':\n",
    "    optimizer = torch.optim.SGD(mlp_model.parameters(), lr=config.learning_rate_mlp, momentum=config.sgd_momentum,\n",
    "                                     dampening=config.sgd_dampening, weight_decay=config.sgd_weight_decay, nesterov=config.sgd_nesterov) # momentum=0.9, weight_decay=0, nesterov=false\n",
    "elif config.mlp_optimizer == 'adam':\n",
    "    # self.optimizer = torch.optim.Adam(self.mlp_model.parameters(), lr=config.learning_rate_mlp)\n",
    "    optimizer = torch.optim.Adam(mlp_model.parameters(), lr=config.learning_rate_mlp,\n",
    "                                      weight_decay=config.adam_weight_decay, amsgrad=config.adam_amsgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other variables\n",
    "SMALL = float('-inf')\n",
    "TASK_num = Y_train.shape[0]\n",
    "TRAIN_size = Y_train.shape[1]\n",
    "\n",
    "if config.using_mlp:\n",
    "    if config.hidden_dim_2 != 0:\n",
    "        d = config.hidden_dim_2\n",
    "    else:\n",
    "        d = config.hidden_dim_1\n",
    "else:\n",
    "    d = config.input_dim\n",
    "with torch.no_grad():\n",
    "    if config.w_from_matlab:\n",
    "        W_mat = scipy.io.loadmat(config.train_weight_location)\n",
    "        W = torch.tensor(W_mat['W']).float()\n",
    "    else:\n",
    "        W = torch.nn.Parameter(\n",
    "            torch.empty(d, TASK_num).normal_(mean=0, std=0.1))  # default: 0.1 # compare this with matlab\n",
    "\n",
    "    values = Y_train.unique()\n",
    "    k = values.shape[0]\n",
    "    pi = torch.nn.Parameter(torch.ones(k - 1, 1) * 1 / k)\n",
    "    cum_pi = torch.cumsum(pi, dim=0)\n",
    "    T = torch.log(cum_pi / (1 - cum_pi))\n",
    "    T = T.repeat(1, TASK_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(TASK_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for adi in range(1):\n",
    "    for t in range(2):\n",
    "        print(adi, t)\n",
    "        loss = 0\n",
    "        if config.using_mlp:\n",
    "            optimizer.zero_grad() # optimizer\n",
    "            X_hidden_init = mlp_model(X_train)\n",
    "        for i in range(TASK_num):\n",
    "            for j in range(TRAIN_size):\n",
    "                X_hidden = X_hidden_init[i,j,:]\n",
    "                if Y_train[i,j]==k:\n",
    "                    sig1=1\n",
    "                    # sig2= logsig(T[int(Y_train[i,j])-2,i] - torch.matmul(X_hidden,W[:,i])) # logsig result is not same as matlab\n",
    "                    sig2= 1 / (1 + torch.exp(-(T[int(Y_train[i,j])-2,i] - torch.matmul(X_hidden,W[:,i]))))\n",
    "                    # sig2 = 1 / (1 + torch.exp(-(T[int(Y_train[i, j]) - 2, i] + torch.matmul(X_hidden, W[:, i])))) # Bug warning: according to eq1 it may be +\n",
    "                    dif=SMALL # default: SMALL (Bug Warning: Not sure if it is BIG or not)\n",
    "                elif Y_train[i,j]==1:\n",
    "                    # sig1= logsig(T[int(Y_train[i,j])-1,i] - torch.matmul(X_hidden,W[:,i])) # Bug Warning: why not just sigmoid? Equ 1 paper\n",
    "                    sig1= 1 / (1 + torch.exp(-(T[int(Y_train[i,j])-1,i] - torch.matmul(X_hidden,W[:,i]))))\n",
    "                    sig2=0\n",
    "                    dif=SMALL\n",
    "                else:\n",
    "                    # sig1= logsig2(T[int(Y_train[i,j])-1,i] - torch.matmul(X_hidden,W[:,i]))\n",
    "                    sig1= 1 / (1 + torch.exp(-(T[int(Y_train[i,j])-1,i] - torch.matmul(X_hidden,W[:,i]))))\n",
    "                    # sig1 = 1 / (1 + torch.exp(-(T[int(Y_train[i, j]) - 1, i] + torch.matmul(X_hidden, W[:, i])))) # Bug warning: according to eq1 it may be +\n",
    "                    # sig2= logsig2(T[int(Y_train[i,j])-2,i] - torch.matmul(X_hidden,W[:,i]))\n",
    "                    sig2= 1 / (1 + torch.exp(-(T[int(Y_train[i,j])-2,i] - torch.matmul(X_hidden,W[:,i]))))\n",
    "                    # sig2 = 1 / (1 + torch.exp(-(T[int(Y_train[i, j]) - 2, i] + torch.matmul(X_hidden, W[:, i])))) # Bug warning: according to eq1 it may be +\n",
    "                    dif= (T[int(Y_train[i,j])-2,i]-T[int(Y_train[i,j])-1,i])\n",
    "                # loss\n",
    "\n",
    "                l_cum=-torch.log(sig1-sig2)\n",
    "                loss=loss+l_cum\n",
    "#                 print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "fc1.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "for name, param in mlp_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "#         print(name)\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mlp_model(X_train)-X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for adi in range(1):\n",
    "    for t in range(2):\n",
    "        print(adi, t)\n",
    "        loss = 0\n",
    "        if config.using_mlp:\n",
    "            optimizer.zero_grad() # optimizer\n",
    "#             X_hidden_init = mlp_model(X_train)\n",
    "        for i in range(TASK_num):\n",
    "            for j in range(TRAIN_size):\n",
    "#                 X_hidden = X_hidden_init[i,j,:]\n",
    "                X_hidden = mlp_model(X_train[i,j,:])\n",
    "                if Y_train[i,j]==k:\n",
    "                    sig1=1\n",
    "                    # sig2= logsig(T[int(Y_train[i,j])-2,i] - torch.matmul(X_hidden,W[:,i])) # logsig result is not same as matlab\n",
    "                    sig2= 1 / (1 + torch.exp(-(T[int(Y_train[i,j])-2,i] - torch.matmul(X_hidden,W[:,i]))))\n",
    "                    # sig2 = 1 / (1 + torch.exp(-(T[int(Y_train[i, j]) - 2, i] + torch.matmul(X_hidden, W[:, i])))) # Bug warning: according to eq1 it may be +\n",
    "                    dif=SMALL # default: SMALL (Bug Warning: Not sure if it is BIG or not)\n",
    "                elif Y_train[i,j]==1:\n",
    "                    # sig1= logsig(T[int(Y_train[i,j])-1,i] - torch.matmul(X_hidden,W[:,i])) # Bug Warning: why not just sigmoid? Equ 1 paper\n",
    "                    sig1= 1 / (1 + torch.exp(-(T[int(Y_train[i,j])-1,i] - torch.matmul(X_hidden,W[:,i]))))\n",
    "                    sig2=0\n",
    "                    dif=SMALL\n",
    "                else:\n",
    "                    # sig1= logsig2(T[int(Y_train[i,j])-1,i] - torch.matmul(X_hidden,W[:,i]))\n",
    "                    sig1= 1 / (1 + torch.exp(-(T[int(Y_train[i,j])-1,i] - torch.matmul(X_hidden,W[:,i]))))\n",
    "                    # sig1 = 1 / (1 + torch.exp(-(T[int(Y_train[i, j]) - 1, i] + torch.matmul(X_hidden, W[:, i])))) # Bug warning: according to eq1 it may be +\n",
    "                    # sig2= logsig2(T[int(Y_train[i,j])-2,i] - torch.matmul(X_hidden,W[:,i]))\n",
    "                    sig2= 1 / (1 + torch.exp(-(T[int(Y_train[i,j])-2,i] - torch.matmul(X_hidden,W[:,i]))))\n",
    "                    # sig2 = 1 / (1 + torch.exp(-(T[int(Y_train[i, j]) - 2, i] + torch.matmul(X_hidden, W[:, i])))) # Bug warning: according to eq1 it may be +\n",
    "                    dif= (T[int(Y_train[i,j])-2,i]-T[int(Y_train[i,j])-1,i])\n",
    "                # loss\n",
    "\n",
    "                l_cum=-torch.log(sig1-sig2)\n",
    "                loss=loss+l_cum\n",
    "#                 print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "fc1.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "for name, param in mlp_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "#         print(name)\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mlp_model(X_train)-X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Incomplete_label_multi_task]",
   "language": "python",
   "name": "conda-env-Incomplete_label_multi_task-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
